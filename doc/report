# Final Project Report
## Introduction
So, why sign up for a compiler course? After all, there are only a
handful of ubiquitous programming languages in the industry today. The
chances that the modern programmer will ever have to write a full-featured
compiler are quite low. The closest one might get is writing a compiler
for a DSL. But we did not choose this compiler course only for the
practical experience of writing a compiler, we chose this course because
we all had a deep desire to learn more about one of the most fundamental
tools in computer science. We also knew that one of the best ways to
fully understand a concept is to deconstruct it and rebuild it. This
course provided that very opportunity.

Unlike many courses, we were given significant latitude in our work,
designing and implementing a project of our choosing. Of course, the
question then is, what would we like to do? There are several criteria
we could apply here. We need to balance the feasibility of our project
against what we would actually like to implement. We also needed to decide
if we wanted to create our own language or to reimplement an existing one.

After much deliberation, we decided on implementing an existing
language, Tiger, so we could focus on writing the compiler. Tiger
isn't prohibitively complicated, but it still has enough issues to be
a challenge. Tiger is a simple, general purpose language; it supports
several of the interesting features of more complex languages, but little
of the bloat. This is helpful in two ways. First, it makes writing the
compiler easier. Secondly, it gives us the option to expand the language
with constructs of our design if we so desire. As it turned out, we did
not have enough time to work on extensions because we ran into several
unforeseen issues during the development process. We will elaborate more
on these issues later.

## Language Tutorial
## Language Reference Manual
## Project Plan
Since our group was smaller than the other groups, we were able to operate
with a fairly loose organization. We maintained a simple overarching
goal: to write a working compiler. We steadily progressed toward our
goal throughout the semester. We had a general idea of what needed to
be accomplished but didn't want to get bogged down with details from
the beginning.

Our group hierarchy was mostly flat and decisions were made
democratically. Group roles were shared among us fairly equally. We had
scheduled meetings on Tuesdays and Thursdays, but the actual meeting
schedule changed from week to week according to the current state of
the project. We briefly met after class to decide whether a meeting
was necessary, whether it needed to be rescheduled, or if we possibly
needed to schedule additional meetings that week. In addition to physical
meetings, we used emails and text messaging to coordinate activity and
discuss issues.

At the beginning of the project, the things we needed to do were fairly
straightforward. When we were working on the initial documentation and
the lexer/parser, it was easy to divide these into tasks that people could
work on separately. However, once we started working on the intermediate
parts of the compiler, this strategy started to fall apart. We divided
this work into sections as before, but the phases of the compiler were
more interdependent than we realized. We couldn't design good interfaces
between the modules until we all had a good understanding of all the
issues.

Due to our flexible group structure, we were able to reorganize and try
a new approach. We focused on lots of group discussion to try to isolate
and resolve as many issues as possible before creating well defined
contracts that needed to be met by each phase of the compiler. This was
still a difficult task and required several revisions, but through better
communication we were able to make steady progress. Towards the end of
the project, we realized that the most effective approach involved not
only group discussion, but actual group coding. We were able to help
each other and work out issues as they arose.

## Language Evolution
Not applicable.

## Compiler Architecture
### The Problem
The architecture of a compiler is heavily influenced by the particular
problem posed by the differences between its source and target
languages. Thus, to understand the rationale behind our compiler
architecture, you must understand the big differences between Tiger and
C. There were two main differences that caused the most trouble throughout
the design of the compiler. The obvious difference is that Tiger allows
nested functions, while C does not. A nested function in Tiger can see and
use all the declarations made in its surrounding scope. Another problem
that may not be clear at first is that Tiger makes no distinction between
statements and expressions in C. Each of these problems was known to
some to degree at the beginning of the project, but their implications
were not fully realized until an implementation was attempted.

### Design Evolution
Our first attempt was to handle all the issues with just the semantic
analysis phase. Initially, we planned on having just two representations,
the AST and a C subset, then performing all the transformations in
a single pass. We quickly realized how much more complicated the
transformations would need to be, and that we would need to break the
work up into multiple transformations.

To address the problems of our initial plan, we decided to perform several
passes on the AST, each with a specific goal. Our second attempt at a
compiler architecture consisted of the following passes:

- Semantic Analysis
	Responsible solely for type-checking.

- Let Coalescing
	This pass assumes that the given AST is semantically
	correct. After the transformation each function (and the top-level
	expression) will start with a let expression that contains all
	variable, type, and nested function declarations. After this
	transformation there will be no nested lets.

- Activation Records
	This pass assumes that each function contains a single let
	expression as its body. After the transformation, all the local
	variables for each function are moved into a Tiger-language
	record. This record might also contain a reference to the
	enclosing functions activation record. After this record is
	built, all variable access needs to be changed to go through
	the AR instead. Also, nested functions need to be passed their
	parent's AR as an extra argument. After this pass, each function
	has a single activation record, which is its only local variable.

- Globalization
	This pass assumes that each function has no free variables,
	which is handled by the previous pass. Each function is made
	global by moving it to the top-level let, which serves as a
	global namespace. After this pass, there is a single top-level let
	expression that contains all type and function declarations. Each
	function contains exactly one let expression with just the
	variable declaration of it's activation record.

This approach had some significant problems. Maintaining all these
assumptions between transformations is complicated and error-prone. Also,
there were situations were we needed to represent things that aren't
valid in Tiger.

	- The let-expression of a function with no local variables
	  contains an empty declaration list.
	- We need to maintain the order in which variables are
	  initialized, but we also need to move all variable declarations
	  to one place. This required a hack on the AST to support
	  uninitialized variables, and then we had to make initializations
	  into explicit assignments.

These are all enormous hacks, so we decided to make a separate
representation for each transformation. The idea was to make new data
structures so we could more easily represent and enforce the assumptions
of the transformation. The representations we came up with were just
variations on the AST. This was an improvement, but it still had
significant limitations. Defining so many representations was a lot of
code, and it was hard to keep track of everything. Worse, the AST-like
representations were really difficult to manipulate.

Eventually, we had the idea (inspired by the MLton implementation) of
using symbol renaming to make all names globally unique. This allowed
us to store information about each compiler object in a big set of
tables. Once we started using this, a lot of problems disappeared. We
were able to simplify down to just two representations: the AST and
the IR. Let coalescing was handled naturally by the semantic analysis,
we stopped using activation records and switched to lambda lifting,
and globalization is a non-issue since functions are all separate data
structures anyway.

This mostly worked, but we ran into some more problems when we started
doing the code generation. At first, we thought we could deal with the
expression/statement issues during this phase, but it turned out to be
more complicated than we thought. We decided to move that complexity to
another transformation on the IR, but ran into the same problems we had
with the AST: The code generation needed to make too many hard-to-enforce
assumptions about the IR, and we started needing to add hacks to the IR
to represent things.

To resolve this, we made another representation, the CIR, which is similar
the IR with some additional restrictions. It makes a distinction between
statements and expressions, contains no loops (goto/label instead),
doesn't support nested functions, and arrays and records must be
explicitly allocated and initialized.

### Transformations
The final compiler architecture consists of six phases and three data
structures. The lexer and parser transformed the source code file into an
AST representing the structure of the Tiger program. These transformations
were fairly straightforward and do not warrant much discussion.

The semantic analysis was responsible for making each name unique,
resolving references, and creating the IR. The construction of the
IR involves getting rid of the concept of let expressions, which are
replaced with blocks. A block corresponds to a function and contains all
of the local variables and the expression body for that function. Blocks
also keep track of which blocks are nested in them and which block they
are nested in (if any). A top-level block is created that contains all
variable declarations and code that is not in any function.

The next transformation is lambda lifting, which which removes all block
nesting. To do this, we must replace all non-local (free) variables with
pass-by-reference arguments. This involves modifying a function's argument
list and any calls to that function. We also need to add support to the
IR for pass-by-reference arguments so that we can write to non-local
variables.

We based our lambda-lifting algorithm on the one from the Wikipedia page,
it works as follows:

.1. Replace each free variable in each block with an additional argument
    to the enclosing function. Modify each call to that function to pass
    in the extra arguments.
.2. Repeat until all there are no free variables.

We call the last major transformation in our compiler "c-ification." It
transforms the IR, which still does not distinguish between expressions
and statements, to the CIR. This is actually harder than it sounds. It
first involves making array and record initialization and allocation
explicit.  Similarly, each record must be explicitly allocated
and initialized. In Tiger, array and record literals may be used as
expressions and, say, passed to a function. Since this includes implicit
initialization, we must replace the literal with a temporary value
beforehand, explicitly assign a value to this temporary, and lastly
pass the temporary to the function. Finally, we must handle loops
and conditionals. In C, these are statements, but in Tiger these are
expressions. What this means is that, for example, a for loop may occur
inside a if condition. This makes no sense in C, so we need to move the
for loop out. Any other weird combination must be similarly handled,
either by moving expressions out (or in the case of loops) in.

Once we build the final representation, it's fairly straightforward to
generate C source code. We generate code in the following manner:

	- Include <stdio.h>, <stdlib.h>, and <string.h>
	- Print out the Tiger standard library.
	- Forward declarations for arrays and records as well as function
	  prototypes. This is needed to support mutually recursive
	  records and functions.
	- Output structure declarations for arrays and records.
	- Output function definitions.
	- Output the C-language entry point: main

## Development Environment
Tool choice has a big impact on a project, but we had a pretty solid
idea going into the class about what tools we wanted to use. We wrote
our compiler in Standard ML (SML), built with make, and managed our
code-base with git.

SML is a particularly expressive functional language. Like other
high-level languages, it abstracts away issues such as memory management
and type declarations, letting the programmer focus on the actual
problem at hand. But how well suited is SML for the particular problem
of compiler writing?

Because of the scale and complexity of compilers, an implementation
language should have a strong module system, allowing the programmer
to develop and test functional units separately. The SML module system
is very different from the class-based one you would find in Python or
Java. SML uses structures which are collections of types and values,
signatures which are contracts for structures, and functors which are
parameterized structures. Functors are used in a way similar to C++
templates, but don't have the all the messy complexity problems.

SML also supports an excellent type system that allows users to
quickly declare and use new types. In traditional SML style, one uses
types to enforce design contracts. As an example, in our compiler, we
represent all of our intermediate forms with a datatype. When we write
a function to construct the syntax tree, there is no way to construct
it invalidly. This cuts back significantly on the number of special
cases we need to handle. What that means for the programmer: if the
program compiles, one knows immediately that it is correct, not just
syntactically, but semantically. This approach is often referred to as
"Make invalid states unrepresentable."

Like most functional languages, SML makes using recursion very
natural. This matches perfectly with the deeply recursive data-structures
used by compilers. Using the pattern matching facilities and parametric
datatypes, a developer can easily recurse through complex data structures
with only a few lines of code. As an example, we wrote a function to
walk through our abstract syntax tree, applying an arbitrary function to
each element and accumulating the results. This required only 30 lines of
code. To do this using the object-oriented paradigm in C++, each element
would need a separate class derived from the same base class, essentially
repeating the same block of code over and over again save one function,
which would provide the required polymorphic functionality. And that
doesn't even allow for parametricity, which would require even more code,
i.e. a template.

We've included an example to show how easy it is to manipulate expressions
trees with SML.  It just defines an AST for a simple language, implements
a simple evaluator (`eval'), and invokes the evaluator for a small
example expression.  Imagine how much code this would need in C++.

| datatype value = I of int | S of string | VOID
| datatype exp = PLUS of exp * exp | MUL of exp * exp
|              | LITERAL of value | PRINT of exp
|
| exception TypeError of exp
|
| fun eval (exp:exp) =
|  case exp
|   of LITERAL l => l
|    | PLUS (op1,op2) =>
|      (case (eval op1, eval op2)
|         of (I i1, I i2) => I(i1+i2)
|          | (S s1, S s2) => S(s1^s2)
|          | _ => raise TypeError exp)
|    | MUL (op1,op2) =>
|       (case (eval op1, eval op2)
|         of (I i1, I i2) => I(i1*i2)
|          | _ => raise TypeError exp)
|    | PRINT e =>
|       ( case eval e
|          of I i => print(Int.toString i)
|           | S s => print("\"" ^ s ^ "\"")
|           | VOID => print "void"
|       ; print "\n"
|       ; VOID
|       )
|
| ;
| eval
|  (PRINT
|    (PLUS ((LITERAL (S "hi ")),
|           (LITERAL (S "there")))));

But a good development environment is more than just a language. One also
needs a good debugger. Fortunately, there is an SML implementation known
as SML/NJ which provides an interactive SML development environment. Small
code snippets can be written directly on the command line and executed
interactively. This allows the programmer to test functions immediately
after writing them by loading them into the environment and then providing
sample test inputs, avoiding a lengthy compilation process and extra
test input files.

Since the functional style doesn't use global state, functions can usually
be tested separately from the rest of the code. Usually, when you write
a new function you can just copy-paste it into the interpreter and throw
some inputs at it. You don't need to setup a bunch of global state and
instantiate interdependent objects just to test a method.

To manage our releases, we used a simple set of makefiles. Though make
is often supplemented with additional tools, it is more than capable of
managing complex builds on its own, especially given the fact that one
can call shell commands to provide make with any programmability it does
not contain internally. Actually, we used the lesser known plan9 mk. It
is basically the same idea as make, but with cleaner syntax and semantics.

To perform code sharing, we chose Git. Firstly, Git uses a distributed
architecture where everyone maintains a complete copy of the source
tree. This allows for lightning-fast commits, because a commit is simply
dumping metadata to the filesystem. In order to share code and merge data,
a developer need only pull changes from another repository and merge it
with his own changes. In addition, Git automatically maintains a commit
history so we know what changed, who did it, and when. Thus, we can easily
roll back changes that we made if we find that bugs were introduced.

To actually write our code, we all used Vim. We all happened to be fairly
competent Vim users and found that we did not need the features offered
by heavier weight IDEs such as Eclipse. Since SML isn't well-supported
by most IDEs, it is questionable how much help they could have offered.

When working on the initial phases of the compiler, we wanted a toolkit
that would abstract away the tedium of hand-rolling our own parser and
lexer. Fortunately, SML has the MLlex and MLyacc libraries, which much
like their C namesake, provide easy-to-use lexing and parsing facilities
specifically for SML. The structure of the files is so similar to the
standard lex and yacc that it is fairly easy to learn one format given
the other.

Probably the last tool we used which is worth noting is Etherpad, which
we used to write this document. It is a distributed plaintext document
editing framework. It allows several users to work on the same document
simultaneously by highlighting the changes each user makes in separate
colors. This allows changes to be carefully tracked and managed by
the group. It greatly simplified our documentation writing process,
especially given the distributed nature of the final report and the
various sections it required.

## Test Plan and Test Suites
Testing is an integral phase of any software project. Many projects
start with excellent momentum and have thousands of lines of code written
towards them, but suffer as they approach maturity because the developers
failed to adequately test the components individually early on. The end
result is a pile of half-working code that is so convoluted it is nearly
impossible to debug (or at least, to do so without losing one's sanity).

To allow for immediate testing, the compiler can be run in a debug mode,
which causes the current internal representation of the Tiger program
to be output after each phase of the compiler completes its respective
tasks. This allows the tester to verify that the compiler is performing
as desired and the proper transformations are being performed on the
internal representations. To implement our debugging mode, we wrote a
small S-expression pretty printer. S-expressions are the core unit of
syntax in the Lisp family of languages, and are simple to generate since
they mirror the hierarchical nature of the syntax tree.

Our core testing strategy consists of three separate test suites. They
each are designed to validate different aspects of the compiler with
each suite being slightly more comprehensive than the previous. The
first test suite consists of a large collection of small, single line
programs that embody the basic features of the language. The test cases
in this suite are designed to exhaustively test the lexical, syntactic,
and semantic analysis phases of the compiler. Targeted features are
correctness of syntax and tokens, type checking, reference resolution,
and proper construction of intermediate forms. Two files comprise this
suite, one with valid input and the other with input designed to fail
at specific phases.

The second test suite has larger test cases that correspond to specific
situations that we deemed likely to cause complications. These
situations include standard and mutual recursion, nested functions
and let expressions, deep nesting of expressions, as well as various
pathological cases that are allowed by Tiger's design. One purpose of
this test suite is to ensure that these complicated cases are handled and
represented correctly throughout the early phases of compilation. Most
of these test cases are also not directly representable in the target
language of C. Thus, another purpose of this test suite is to ensure the
correctness of several phases of the compiler, specifically lambda lifting
and c-ification, that have the task of transforming these situations
into equivalent forms that are suitable to being represented in C.

The final test suite is a set of complete programs. This provides a
final comprehensive verification of the entire compiler. The generated
code can then be run and compared against the expected behavior of the
original Tiger program. Unfortunately, there is no way to automatically
generate the expected output since we do not have an existing compiler
implementation to test against. These hand generated test files must
be carefully inspected so that they remain bug free. Otherwise, subtle
errors might slip through our test case.

## Conclusions
### Aaron
This was the first major group project I have been a part of and it
has been a great experience for me. It has taught me that constant
communication in a group is very important. I've learned how to better
interact with other people to accomplish a common goal efficiently. It has
given me a chance to practice explaining my thoughts and ideas in a clear
and effective manner as well as listening to ideas of others. Following
and understanding a complex train of thought on a complicated topic that
someone else is trying to explain is not easy, but is a valuable skill
to possess.

In addition, I have learned a new language, SML, as well as gained a deep
appreciation of the power and joy of functional programming. Learning
a new programming paradigm involves becoming familiar with an entirely
new way of thinking about computation and problem solving, as well as
new terms and concepts. I have also learned how to use Git and will
likely begin to use it for my own projects. The process of designing and
implementing a compiler has given me a better understanding of programming
languages in general and how their features affect their implementation. A
compiler has always been sort of a black box to me, but now that mystery
is gone. This project has helped me grow as a programmer and has been
an excellent intellectual exercise. The people in my group have also
taught me numerous small things throughout the semester.

### Ben
I've been part of a handful of group projects before. I think the biggest
thing I learned is how much nicer it is to work with a good group. All
the other groups I've been in have had at least a couple of people who
weren't motivated at all; it is always a pain to deal with. It truly is
worth the time it takes to hunt down good people.

I've also gained some experience dealing with designing data
representations. It's difficult to know where to draw the line between
what should be enforced by the language and what should be enforced by the
documentation and by the programmers. Too much specification is more work
than it's worth, but too little specification makes things unmanageable.

### Stephen
The things I've learned from this project are numerous. First of all,
I've learned that I need to be diligent in how I manage my time. In this
project, I did not do as much work as the beginning as I should have,
causing me to have to rush towards the end to get work done.

Also, when we were initially deciding what tools to use and our
implementation language, I did not voice my vote as loudly as I should
have. I was not as familiar with SML as I should have been before
starting, and I was handicapped by that inability. Fortunately, as time
went on, I was able to learn how to use SML fairly well, and now I sort
of enjoy programming in it.

This was my first attempt at a project of this magnitude, and also my
first attempt to write a real program in a functional language. So,
I learned a TON about how functional languages work, how to write
functional data structures, how to do pattern matching, how to make
proper use of recursion, etc...

I think I also became really familiar with lex and yacc over the course
of project (and decided that I like them a lot), because they are so easy
to use and generate lots of tedious code for me that I would otherwise
have to write myself.

Lastly, I've come to appreciate just how hard writing a compiler is. They
are incredibly complex and full of places where one wrong assumption
will cause hours of wasted work. Even though I enjoyed it, I don't know
if I want to have to do this again for awhile...

### Group
We've tried various approaches to organizing a group, and are convinced
that working together, in person, is a huge productivity win. It's really
hard to maintain good communication using email and short meetings. Still,
being productive as a group is not always easy. One of the advantages
of being together as a group is that it's really easy to stop someone
to discuss an issue, but it's possible to overdo this. It takes some
practice to find a good balance. Some of this is specific to a given
group because individuals think and communicate differently.

The other main point we learned (through experience) is that compromise is
hard. We are all opinionated people who are passionate about our subject.
We all had to accept the fact that we are different; we want different
things out of this class and think differently about the project. At the
same time, compromise is essential. Without it, we would never have been
able to get anything done.
